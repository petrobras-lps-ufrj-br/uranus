# ðŸ—„ Triton Model Repository Structure

Triton Inference Server requires a specific directory structure to discover and load models. Below is the proposed schema for **Uranus**:

```text
data/model_repository/
â”œâ”€â”€ <model_name>/
â”‚   â”œâ”€â”€ config.pbtxt        # Model configuration (backend, inputs, outputs)
â”‚   â”œâ”€â”€ 1/                  # Version number (must be numeric)
â”‚   â”‚   â””â”€â”€ model.pt        # Model file (e.g., PyTorch, ONNX, etc.)
â”‚   â””â”€â”€ 2/                  # (Optional) Newer version
â”‚       â””â”€â”€ model.pt
â””â”€â”€ sample_model/
    â”œâ”€â”€ config.pbtxt
    â””â”€â”€ 1/
        â””â”€â”€ model.pt
```

### ðŸ“‹ Configuration (`config.pbtxt`)

Example for a PyTorch (LibTorch) model:

```protobuf
name: "sample_model"
platform: "pytorch_libtorch"
max_batch_size: 8
input [
  {
    name: "input__0"
    data_type: TYPE_FP32
    dims: [ 10 ]
  }
]
output [
  {
    name: "output__0"
    data_type: TYPE_FP32
    dims: [ 1 ]
  }
]
```

### ðŸš€ Backend Support

*   **PyTorch**: `pytorch_libtorch` (uses `model.pt`)
*   **ONNX**: `onnxruntime_onnx` (uses `model.onnx`)
*   **TensorFlow**: `tensorflow_savedmodel` (uses `saved_model/`)
*   **Python**: `python` (uses `model.py`)
